---
number: 002
title: Web Crawler Engine Core
status: pending
created: 2025-09-28T15:28:01Z
epic: compliance-keyword-scanner
parallel: true
depends_on: [001]
effort: Large (24-40 hours)
---

# Task 002: Web Crawler Engine Core

## Description

Build the core web crawling engine that discovers website sitemaps, extracts URLs, and fetches page content using Playwright. The crawler must handle various URL structures, navigation patterns, and implement proper rate limiting with comprehensive error handling for robust website scanning.

## Acceptance Criteria

- [ ] Sitemap discovery from robots.txt and standard locations (/sitemap.xml)
- [ ] URL extraction from sitemaps with proper parsing
- [ ] Page content fetching using Playwright with HTML extraction
- [ ] Rate limiting implementation to respect website resources
- [ ] Error handling for unreachable pages and malformed content
- [ ] Support for various URL structures and redirects
- [ ] Crawling progress tracking and status updates
- [ ] Memory management for large websites
- [ ] Unit and integration tests for all crawler functions
- [ ] Documentation for crawler configuration and usage

## Technical Details

### Sitemap Discovery

**Primary Discovery Methods**:
- Check robots.txt for sitemap declarations
- Try standard sitemap locations (/sitemap.xml, /sitemap_index.xml)
- Handle sitemap index files with multiple sitemaps
- Parse XML sitemaps with proper error handling
- Extract URLs with last modified dates and priorities

**URL Processing**:
- Validate and normalize URLs
- Filter URLs based on domain restrictions
- Handle relative vs absolute URL resolution
- Detect and handle URL redirects
- Remove duplicate URLs across sitemaps

### Content Fetching Engine

**Playwright Implementation**:
- Browser instance management with resource optimization
- Page navigation with timeout handling
- HTML content extraction (full DOM access)
- JavaScript rendering for dynamic content
- Screenshot capture for visual verification
- Mobile/desktop user agent rotation

**Resource Management**:
- Concurrent page limit configuration
- Memory usage monitoring and cleanup
- Browser instance pooling
- Request/response logging for debugging

### Rate Limiting & Politeness

**Crawling Behavior**:
- Configurable delay between requests (default: 1-2 seconds)
- Respect robots.txt crawl-delay directives
- Exponential backoff for failed requests
- User-agent identification for website owners
- Request timeout configuration

**Error Handling**:
- Network timeout recovery
- HTTP error status handling (404, 500, etc.)
- Malformed HTML graceful degradation
- SSL certificate issues handling
- DNS resolution failures

### Performance Optimization

**Efficient Crawling**:
- Batch URL processing
- Parallel page fetching with limits
- Content deduplication by hash
- Selective content extraction (text only)
- Compressed content handling

**Progress Tracking**:
- Real-time crawling status updates
- Page count and success/failure metrics
- Estimated completion time calculation
- Queue position and processing rate

## Dependencies

- Task 001 (Database models for storing crawled data)

## Effort Estimate

Large (24-40 hours)

- Sitemap discovery and parsing: 8-12 hours
- Playwright content fetching: 8-12 hours
- Rate limiting and error handling: 4-8 hours
- Performance optimization: 4-8 hours

## Definition of Done

- [ ] Sitemap discovery works for 95% of tested domains
- [ ] Page content extraction handles modern websites
- [ ] Rate limiting prevents server overload
- [ ] Error handling provides meaningful feedback
- [ ] Memory usage remains stable during large crawls
- [ ] Integration tests pass for various website types
- [ ] Crawling progress updates work in real-time
- [ ] Documentation covers all configuration options
- [ ] Performance benchmarks meet requirements (1000 pages in 30 minutes)
